{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4438370",
   "metadata": {},
   "source": [
    "[Перейти к разделу о классификации](Catboost_classifier.ipynb)\n",
    "\n",
    "[Перейти к разделу о регрессии](Catboost_regressor.ipynb)\n",
    "\n",
    "[Перейти к разделу о метриках](Catboost_metrics.ipynb)\n",
    "\n",
    "[Перейти к разделу об обработке текста](text_processing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9a149",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b1721",
   "metadata": {},
   "source": [
    "CatBoost is a machine learning method based on gradient boosting. Initially, it was introduced at Yandex for solving prediction, categorization, and recommendation tasks. CatBoost has built-in support for categorical variables and ensures high accuracy.\n",
    "Main advantages of Catboost\n",
    "1) Superior quality when compared with other GBDT libraries on many datasets.\n",
    "2) Best in class prediction speed.\n",
    "3) Support for both numerical and categorical features.\n",
    "4) Fast GPU and multi-GPU support for training out of the box.\n",
    "5) Visualization tools included.\n",
    "6) Fast and reproducible distributed training with Apache Spark and CLI.\n",
    "\n",
    "Then, on legal grounds, a series of questions arise:\n",
    "1) What are the advantages of CatBoost over its counterparts?\n",
    "2) Why boosting instead of neural networks?\n",
    "3) What do cats have to do with this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeaa1c",
   "metadata": {},
   "source": [
    "# What Catboost can do?\n",
    "    1) Classification\n",
    "    2) Regression\n",
    "    3) Multiclassification\n",
    "    4) Ranging\n",
    "    5) Metrics\n",
    "    6) etc.\n",
    "    \n",
    "# Operating Principle\n",
    "## Decision Tree\n",
    "\n",
    "The operating algorithm is as follows: for each document, there is a set of feature values, and there is a tree with conditions at its nodes. If the condition is met, the algorithm moves to the right child of the node; otherwise, it goes to the left. One needs to traverse the tree to a leaf according to the feature values for the document. The value of the leaf corresponds to the output for each document. That is the answer.\n",
    "\n",
    "Boosting\n",
    "\n",
    "The idea behind the boosting approach is to combine weak (with low generalization ability) functions built during an iterative process, where at each step, a new model is trained using data on the errors of the previous ones. The resulting function is a linear combination of basic, weak models.\n",
    "\n",
    "Next, boosting decision trees will be considered. Several trees will be built, and adding new trees should reduce the error. In total, with a sufficiently large number of trees, the error can be significantly reduced. However, it is essential to remember that the more trees, the longer the model takes to train, and at some point, the quality improvement becomes insignificant.\n",
    "Gradient Boosting\n",
    "\n",
    "## CatBoost is based on gradient boosting.\n",
    "\n",
    "The gradient of the error function includes all derivatives with respect to all values of the function.\n",
    "\n",
    "Gradient boosting is a machine learning method that creates a predictive model in the form of an ensemble of weak prediction models, usually decision trees. It builds the model step by step, allowing the optimization of any differentiable loss function.\n",
    "\n",
    "## Gradient boosting\n",
    "\n",
    "Boosting is a method which builds a prediction model $F^{T}$ as an ensemble of weak learners $F^{T} = \\sum\\limits_{t=1}^{T} f^{t}$.\n",
    "\n",
    "In our case, $f^{t}$ is a decision tree. Trees are built sequentially and each next tree is built to approximate negative gradients $g_{i}$ of the loss function $l$ at predictions of the current ensemble:\n",
    "$g_{i} = -\\frac{\\partial l(a, y_{i})}{\\partial a} \\Bigr|_{a = F^{T-1}(x_{i})}$\n",
    "Thus, it performs a gradient descent optimization of the function $L$. The quality of the gradient approximation is measured by a score function $Score(a, g) = S(a, g)$.\n",
    "\n",
    "# Features of CatBoost\n",
    "## Operating Modes\n",
    "\n",
    "- Regression\n",
    "- Classification\n",
    "\n",
    "#### Loss Function: Maximizes the probability that all objects in the training set are classified correctly, where probability is the sigmoid function applied to the formula's value.\n",
    "\n",
    "#### predict_proba Function: Outputs ready probabilities. It's important to note that these probabilities cannot be summed.\n",
    "\n",
    "#### predict Function: Outputs raw results. Such results can be combined, for example, with the results of other models.\n",
    "\n",
    "- Multiclass Classification\n",
    "\n",
    "- Ranking\n",
    "\n",
    "## Metrics\n",
    "\n",
    "CatBoost supports a variety of metrics, such as:\n",
    "\n",
    "    Regression: MAE, MAPE, RMSE, SMAPE, etc.\n",
    "    Classification: Logloss, Precision, Recall, F1, CrossEntropy, BalancedAccuracy, etc.\n",
    "    Multiclass Classification: MultiClass, MultiClassOneVsAll, HammingLoss, F1, etc.\n",
    "    Ranking: NDCG, PrecisionAt, RecallAt, PFound, PairLogit, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c8032",
   "metadata": {},
   "source": [
    "<span style=\"display:none\" id=\"q_demo_seq\">W3sicXVlc3Rpb24iOiAiXHUwNDFmXHUwNDQwXHUwNDNlXHUwNDM0XHUwNDNlXHUwNDNiXHUwNDM2XHUwNDM4XHUwNDQyXHUwNDM1IFx1MDQzZlx1MDQzZVx1MDQ0MVx1MDQzYlx1MDQzNVx1MDQzNFx1MDQzZVx1MDQzMlx1MDQzMFx1MDQ0Mlx1MDQzNVx1MDQzYlx1MDQ0Y1x1MDQzZFx1MDQzZVx1MDQ0MVx1MDQ0Mlx1MDQ0YzogJDIkLCAkXFxmcmFjIDk0JCwgJFxcZnJhY3s2NH17Mjd9JCwgPyIsICJ0eXBlIjogIm51bWVyaWMiLCAiYW5zd2VycyI6IFt7InR5cGUiOiAidmFsdWUiLCAidmFsdWUiOiAyLjQ0MTQwNjI1LCAiY29ycmVjdCI6IHRydWUsICJmZWVkYmFjayI6ICJcdTA0MTJcdTA0MzVcdTA0NDBcdTA0M2RcdTA0M2UhIFx1MDQyZFx1MDQ0Mlx1MDQzZSBcdTA0M2ZcdTA0M2VcdTA0NDFcdTA0M2JcdTA0MzVcdTA0MzRcdTA0M2VcdTA0MzJcdTA0MzBcdTA0NDJcdTA0MzVcdTA0M2JcdTA0NGNcdTA0M2RcdTA0M2VcdTA0NDFcdTA0NDJcdTA0NGMgJFxcYmlnKFxcZnJhY3tuKzF9blxcYmlnKV5uJCwgXHUwNDNmXHUwNDQwXHUwNDM4ICRuPTQkIFx1MDQzZlx1MDQzZVx1MDQzYlx1MDQ0M1x1MDQ0N1x1MDQzMFx1MDQzNVx1MDQzYyAkXFxmcmFjezVeNH17NF40fSA9IFxcZnJhY3s2MjV9ezI1Nn0gPSAyLjQ0MTQwNjI1JCJ9LCB7InR5cGUiOiAiZGVmYXVsdCIsICJmZWVkYmFjayI6ICJcdTA0MWVcdTA0NDJcdTA0MzJcdTA0MzVcdTA0NDIgXHUwNDNkXHUwNDM1XHUwNDMyXHUwNDM1XHUwNDQwXHUwNDNkXHUwNDRiXHUwNDM5ISBcdTA0MTdcdTA0MzAgXHUwNDNmXHUwNDNlXHUwNDM0XHUwNDQxXHUwNDNhXHUwNDMwXHUwNDM3XHUwNDNhXHUwNDNlXHUwNDM5IFx1MDQzZlx1MDQ0MFx1MDQzZVx1MDQzYlx1MDQzOFx1MDQ0MVx1MDQ0Mlx1MDQzMFx1MDQzOVx1MDQ0Mlx1MDQzNSBcdTA0M2RcdTA0MzVcdTA0M2NcdTA0M2RcdTA0M2VcdTA0MzNcdTA0M2UgXHUwNDMyXHUwNDRiXHUwNDQ4XHUwNDM1In1dfV0=</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f79dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23fe972",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
